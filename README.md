# Парсер ParsoTangue 

Грамматика языка разделена на 2 части: лексическая, в которой терминалами являются символы разбираемого текста, 
и синтаксическая, в которой терминалами являются токены(некоторые из нетерминалов лексической грамматики).

Парсер, соответственно, также разбит на 2 части: Lexer, разделяющий входной текст на список токенов, 
и Parser, принимающий на вход список токенов и выдающий AST.

Для удобства, создан класс `ParsoTangueParser`, имеющий статические методы для парсинга программы из строки, файла и произвольного `CharSource`.

Помимо функциональности, описанной в задании, была добавлена возможность писать комментарии (в синтаксисе, совпадающем с многострочными комментариями в Java/C, т.е. начинаются с `/*` и заканчивается в `*/`), а также унарные `+` и `-`.

## Грамматики
[Лексическая](./lexical_grammar.bnf), [Синтаксическая](./syntactic_grammar.bnf)

Расширение файла `.bnf` и текст внутри не везде в стандартном синтаксисе EBNF, 
чтобы была подсветка синтаксиса в [IntelliJ IDEA](https://www.jetbrains.com/ru-ru/idea/) 
с плагином [Grammar-Kit](https://github.com/JetBrains/Grammar-Kit).

Более подробные пояснения ниже в тексте:

## Лексическая грамматика и Lexer

В правиле для нетерминала `<whitespace>` записано `"regexp:\w"`, имеется ввиду все символы,
удовлетворяющий условию [java.lang.Character.isWhitespace](https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/lang/Character.html#isWhitespace(char)), объединённые через `|`.

В правиле для нетерминала `<comment_tail>` записано `"regexp:[^*]"`, имеется ввиду все символы, кроме `*`, объединённые через `|`,
также записано `"regexp:[^*/]"`, имеется ввиду все символы, кроме `*` и `/`, объединённые через `|`.

В правиле для нетерминала `<integer_literal>` есть `"regexp:[0-9]"`, имеется ввиду все цифры от 0 до 9, объединённые через `|`(отрицательные литералы не поддерживаются(вместо них можно использовать унарный минус), числа, начинающиеся с нуля, разрешаются).

В правиле для нетерминала `<string_literal>` есть `"regexp:[^\"]"`, имеется ввиду все символы, кроме `"`, объединённые через `|`.

В правиле для нетерминала `<ident>` есть `"regexp[a-zA-Z_]"`, имеется ввиду все буквы латинского алфавита и символ `_`, объединённые через `|`. Также в этом правиле есть символ `!`, имеется ввиду `-` из EBNF, т.е. `<ident>` это непустая последовательность букв латинского алфавита и нижних подчеркиваний, не являющаяся ничем из `<typename>`, `<keyword>`, `<boolean_literal>`.

Lexer разбирает подаваемые ему символы в соответствии с этой грамматикой, при этом `<whitespace>` и `<comment>` не идут в результат, а `<token>` идут, причем они разделяются на разные классы для удобства дальнейшего разбора.

Все классы для токенов лежат в пакете `eliseev.parsotangue.lexer.token`, все остальное связанное с этим этапом разбора лежит в пакете `eliseev.parsotangue.lexer`.

Реализована возможность разобрать `String`, или произвольный `Reader`. Если входная последовательность некорректна, бросается ошибка, содержащая информацию о позиции(номер строки и номер символа в строке) и несколько символов вокруг места обнаружения ошибки.

## Синтаксическая грамматика и Parser

Нетерминалами в грамматике являются токены из результата разбора прошлой грамматикой, но в файле они записаны либо просто как последовательность символов(например, `"let"`, `"("`, `")"` и т.д.), либо текстом (как в `<integer_literal>`, `<string_literal>`, `<ident>`), в этом случае просто указано, что это должен быть токен соответствующего типа из лексической грамматики.

Разделение на 2 грамматики было сделано, чтобы здесь не задумываться о пробелах, потому что могут быть нетривиальные случаи(например, после `return` иногда пробел обязательно должен быть(чтобы отделить его от следующего токена, как в ситуации `return x;`), а иногда он опциональный, потому что токены и так разделимы(как в ситуации `return (3+2);`)).

Бинарные операции разделены на 3 приоритета: умножение/деление/взятие остатка, сложение/вычитание, операции сравнения. При этом на уровне грамматики учтено, что операции сравнения не могут быть соединены в цепочку, потому что их результат - Boolean, а его нельзя ни с чем сравнивать. При этом большая часть странных операций на уровне грамматики являются корректными (например, `true + 10`, `" " > false` и т.п.). Запретить их все не получится, потому что у нас переменные и вызовы функций, которые невозможно разделить по типам(по имени/количеству аргументов нельзя определить тип).

Унарные операции(`+` и `-`) имеют наибольший приоритет.

Все классы для вершин AST лежат в пакете `eliseev.parsotangue.parser.ast`, также на этом этапе разбора используются классы из пакета `eliseev.parsotangue.lexer.token` и сам класс `eliseev.parsotangue.parser.Parser`.

Также токены хранят дополнительную информацию о своей позиции, используемую для вывода ошибок разбора в парсере.

## Тесты

Код тестов лежит в [этой папке](./src/test). Для тестов реализован класс, генерирующий случайную корректную программу на разбираемом языке. Отдельно есть тесты лексера, отдельно парсера(при этом тесты парсера также проверяют и лексер, потому что он все равно запускается). Отдельно можно рассмотреть тесты `writeTokenized`, `writeAST`, `correctFiles` и `incorrectFiles`. Они созданы в основном для демонстрации получаемых списка токенов, AST и вывода ошибок. 
Файлы с текстом для тестов `correctFiles` и `incorrectFiles` и вывод парсера на них находятся в [этой папке](./src/test_files).
Тесты `writeTokenized` и `writeAST` пишут результат в стандартный поток вывода. Тесты `generated` пишут ошибки в стандартный поток ошибок(результат они не пишут, потому что он слишком большой).

Для запуска тестов локально используется библиотека `junit` версии `4.13.1`.